<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AIèµ„è®¯æ—¥æŠ¥ - 2025/8/2</title>
  <style>
    /* æ·»åŠ æŒ‰é’®æ ·å¼ */
    .nav-buttons {
      display: flex;
      gap: 15px;
      justify-content: center;
      margin-top: 15px;
      flex-wrap: wrap;
    }
    .nav-button {
      background: rgba(255, 255, 255, 0.2);
      border: 1px solid rgba(255, 255, 255, 0.4);
      border-radius: 20px;
      padding: 6px 15px;
      color: white;
      text-decoration: none;
      display: inline-flex;
      align-items: center;
      transition: all 0.3s;
      font-size: 0.9em;
    }
    .nav-button:hover {
      background: rgba(255, 255, 255, 0.3);
      transform: translateY(-2px);
    }
    /* å…¶ä»–æ ·å¼ä¿æŒä¸å˜ */
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: #f5f5f5; color: #333; margin: 0; padding: 0; }
    .container { max-width: 1200px; margin: auto; padding: 20px; }
    .header { background: linear-gradient(135deg, #667eea, #764ba2); color: #fff; padding: 40px 20px; border-radius: 10px; text-align: center; }
    .header h1 { font-size: 2.5em; margin: 0; }
    .summary, .history-section { background: #fff; margin-top: 30px; padding: 25px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
    .summary h2, .history-section h2 { color: #667eea; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px; }
    .news-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin-top: 30px; }
    .news-item { background: white; border-radius: 10px; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
    .news-item h3 a { text-decoration: none; color: #333; font-size: 1.1em; }
    .news-item h3 a:hover { color: #667eea; }
    .news-meta { font-size: 0.9em; color: #666; margin-bottom: 10px; }
    .category { display: inline-block; padding: 4px 10px; border-radius: 20px; font-size: 0.8em; margin-right: 8px; }
    .category.industry { background: #e3f2fd; color: #1976d2; }
    .category.academic { background: #f3e5f5; color: #7b1fa2; }
    .category.opensource { background: #e8f5e9; color: #388e3c; }
    .summary-text { margin-top: 10px; color: #555; }
    .importance { background: #ff9800; color: white; padding: 2px 8px; border-radius: 10px; font-size: 0.75em; margin-left: 10px; }
    .history-list { list-style: none; padding-left: 0; }
    .history-list li { margin: 5px 0; }
    .history-list a { text-decoration: none; color: #333; }
    .history-list a:hover { color: #667eea; }
    .footer { text-align: center; font-size: 0.9em; color: #999; padding: 20px; margin-top: 40px; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>ğŸ¤– AIèµ„è®¯æ—¥æŠ¥</h1>
      <p>2025/8/2 | äººå·¥æ™ºèƒ½é¢†åŸŸæœ€æ–°åŠ¨æ€</p>
      <div class="nav-buttons">
        <a href="../daily/2025-08-01.html" class="nav-button">
          ğŸ”™ æŸ¥çœ‹æ˜¨æ—¥å†…å®¹
        </a>
        <a href="../" class="nav-button">
          ğŸ  è¿”å›ä¸»é¡µ
        </a>
      </div>
    </div>

    <div class="summary">
      <h2>ğŸ“Š ä»Šæ—¥è¶‹åŠ¿æ€»ç»“</h2>
      <p>AIé¢†åŸŸæŒç»­å¿«é€Ÿå‘å±•ï¼Œæ¶µç›–äº†ä»ç†è®ºç ”ç©¶åˆ°å®é™…åº”ç”¨çš„å¹¿æ³›è¯é¢˜ã€‚å½“å‰è¶‹åŠ¿æ˜¾ç¤ºï¼Œè¡Œä¸šå†…éƒ¨å¯¹AIç®—æ³•çš„å®é™…åº”ç”¨ç—›ç‚¹ã€æŠ€æœ¯è¿›æ­¥é€Ÿåº¦ã€ä»¥åŠæ³•å¾‹å’Œä¼¦ç†é—®é¢˜è¡¨ç°å‡ºæµ“åšå…´è¶£ã€‚åŒæ—¶ï¼ŒAIæ•™è‚²å’ŒèŒä¸šæœºä¼šï¼Œå¦‚å®ä¹ å’Œç ”ç©¶ç”Ÿæ•™è‚²ï¼Œä¹Ÿå—åˆ°å…³æ³¨ã€‚æŠ€æœ¯åˆ›æ–°ï¼Œå¦‚æ›´ç»æµçš„GPUè§£å†³æ–¹æ¡ˆï¼Œä»¥åŠAIåœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œé¢„ç¤ºç€AIæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</p>
    </div>

    <div class="news-grid">
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=27111719" target="_blank">Ask HN: What's the pain using current AI algorithms?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">æ¢è®¨å½“å‰AIç®—æ³•ä½¿ç”¨ä¸­çš„ç—›ç‚¹ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=36233487" target="_blank">Ask HN: Is the rate of progress in AI exponential?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">è®¨è®ºAIè¿›æ­¥é€Ÿåº¦æ˜¯å¦å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=36431356" target="_blank">Ask HN: Anyone concerned about NYC Local Law 144?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">è¯¢é—®å¯¹çº½çº¦å¸‚åœ°æ–¹æ³•å¾‹144çš„æ‹…å¿§ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://lekta.ai/blog/natural-language-processing-artificial-intelligence-machine-learning-bots-a-passing-trend-or-much-more" target="_blank">NLP, AI, ML, bots â€“ a passing trend or much more? What's your take on this?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">æ¢è®¨NLPã€AIã€MLå’Œæœºå™¨äººæ˜¯çŸ­æš‚è¶‹åŠ¿è¿˜æ˜¯æ›´æ·±è¿œçš„å˜é©ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=401541" target="_blank">Common Lisp + Machine Learning Internship at Google (Mountain View, CA)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">è°·æ­Œæä¾›Common Lispå’Œæœºå™¨å­¦ä¹ å®ä¹ æœºä¼šã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=18049136" target="_blank">50% Cheaper GPUs for cloud-computing / Saving devs 50% compared to AWS</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">æä¾›æ¯”AWSä¾¿å®œ50%çš„GPUäº‘è®¡ç®—è§£å†³æ–¹æ¡ˆã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://www.yourobot.io/blog/uncategorized/the-next-bill-gates-or-albert-einstein-in-ai-artificial-intelligence-will-produce-the-god-algorithm-of-machine-learning-where-a-machine-will-be-able-to-do-and-learn-anything-by-its-self/" target="_blank">The Next Bill Gates or Albert Einstein in AI â€œChris Clarkâ€ â€“ Yourobot</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">ä»‹ç»AIé¢†åŸŸçš„æ½œåœ¨é¢†å†›äººç‰©Chris Clarkã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=15140715" target="_blank">Bioinformatician</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">ç”Ÿç‰©ä¿¡æ¯å­¦å®¶çš„èŒä¸šæœºä¼šã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=12995049" target="_blank">Ask HN: Dipping my toes with artificial intelligence and what to expect? (CS)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 4</span>
          </div>
          <div class="summary-text">åˆå­¦è€…è¯¢é—®æ¶‰è¶³AIé¢†åŸŸçš„é¢„æœŸã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=244100" target="_blank">Ask HN: Thoughts on grad school? (CS PhD)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 4</span>
          </div>
          <div class="summary-text">è®¨è®ºè®¡ç®—æœºç§‘å­¦ç ”ç©¶ç”Ÿæ•™è‚²çš„çœ‹æ³•ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=35103021" target="_blank">The AI Crackpot Index</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 3</span>
          </div>
          <div class="summary-text">AIé¢†åŸŸçš„éä¸»æµè§‚ç‚¹ç´¢å¼•ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=42619160" target="_blank">Show HN: Startup Raising capital through Book Sales</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 2</span>
          </div>
          <div class="summary-text">åˆåˆ›å…¬å¸é€šè¿‡ä¹¦ç±é”€å”®ç­¹é›†èµ„é‡‘ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/jaywalnut310/vits" target="_blank">jaywalnut310/vits</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">VITSï¼šå¸¦æœ‰å¯¹æŠ—å­¦ä¹ çš„æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼Œç”¨äºç«¯åˆ°ç«¯æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7587 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/py-why/dowhy" target="_blank">py-why/dowhy</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">DoWhyæ˜¯ä¸€ä¸ªç”¨äºå› æœæ¨ç†çš„Pythonåº“ï¼Œæ”¯æŒæ˜ç¡®å»ºæ¨¡å’Œæµ‹è¯•å› æœå‡è®¾ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7634 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/jessevig/bertviz" target="_blank">jessevig/bertviz</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">BertVizï¼šå¯è§†åŒ–NLPæ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆBERT, GPT2, BARTç­‰ï¼‰ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7581 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/vwxyzjn/cleanrl" target="_blank">vwxyzjn/cleanrl</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">é«˜è´¨é‡çš„å•æ–‡ä»¶å®ç°æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå…·æœ‰ç ”ç©¶å‹å¥½ç‰¹æ€§ï¼ˆPPO, DQN, C51ç­‰ï¼‰ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7577 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/iamtrask/Grokking-Deep-Learning" target="_blank">iamtrask/Grokking-Deep-Learning</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">æ­¤ä»“åº“ä¼´éšä¹¦ç±â€œGrokking Deep Learningâ€ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7619 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/mikel-brostrom/boxmot" target="_blank">mikel-brostrom/boxmot</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">BoxMOTï¼šä¸ºåˆ†å‰²ã€ç›®æ ‡æ£€æµ‹å’Œå§¿æ€ä¼°è®¡æ¨¡å‹æä¾›å¯æ’æ‹”çš„æœ€å…ˆè¿›å¤šç›®æ ‡è·Ÿè¸ªæ¨¡å—ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7579 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/stanfordnlp/stanza" target="_blank">stanfordnlp/stanza</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">æ–¯å¦ç¦NLP Pythonåº“ï¼Œç”¨äºå¤šç§äººç±»è¯­è¨€çš„æ ‡è®°åŒ–ã€å¥å­åˆ†å‰²ã€NERå’Œè§£æã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7551 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/llSourcell/Learn_Machine_Learning_in_3_Months" target="_blank">llSourcell/Learn_Machine_Learning_in_3_Months</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">è¿™æ˜¯Siraj Ravalåœ¨Youtubeä¸Šå‘å¸ƒçš„â€œ3ä¸ªæœˆå­¦ä¹ æœºå™¨å­¦ä¹ â€çš„ä»£ç ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7633 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/The-Pocket/PocketFlow" target="_blank">The-Pocket/PocketFlow</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">Pocket Flowï¼š100è¡Œä»£ç çš„LLMæ¡†æ¶ã€‚è®©ä»£ç†æ„å»ºä»£ç†ï¼</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7561 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide" target="_blank">ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">ä¸ºæœ¬ç§‘ç”Ÿæˆ–ä»»ä½•å¸Œæœ›æ·±å…¥AIé¢†åŸŸçš„äººæä¾›çš„è¯¦ç»†å’Œå®šåˆ¶åŒ–èµ„æºæŒ‡å—ã€‚</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 7551 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23773v1" target="_blank">SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">é€šè¿‡åŸºäºLLMçš„ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿæ¨ç†æ¶æ„æ„å»ºé€šç”¨ç›®æ ‡å¯¼å‘ä»£ç†ã€‚</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Mingkai Deng, Jinyu Hou, Yilin Shen, Hongxia Jin, Graham Neubig, Zhiting Hu, Eric Xing</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23726v1" target="_blank">Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">LLMs have demonstrated strong mathematical reasoning abilities by leveraging
reinforcement learning with long chain-of-thought, yet they continue to
struggle with theorem proving due to the lack of clear supervision signals when
solely using natural language. Dedicated domain-specific languages like Lean
provide clear supervision via formal verification of proofs, enabling effective
training through reinforcement learning. In this work, we propose
\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover
can iteratively refine its proof based on Lean feedback, proved lemmas, and
self-summarization. To solve IMO-level contest problems, we design three
test-time inference strategies that enable both deep and broad reasoning.
Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F,
and achieves over 50\% on PutnamBench, outperforming the previous
state-of-the-art by a large margin. To address the lack of geometry support in
Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which
outperforms previous formal geometry engines. We use these two systems to
participate in IMO 2025 and fully prove 5 out of 6 problems. This work
represents a significant advancement in automated mathematical reasoning,
demonstrating the effectiveness of formal verification with long
chain-of-thought reasoning.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23784v1" target="_blank">SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">é€šè¿‡åˆæˆå±æ€§æ›¿æ¢è¯„ä¼°æ¦‚å¿µç“¶é¢ˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Jessica Bader, Leander Girrbach, Stephan Alaniz, Zeynep Akata</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23735v1" target="_blank">Distributed AI Agents for Cognitive Underwater Robot Autonomy</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">Achieving robust cognitive autonomy in robots navigating complex,
unpredictable environments remains a fundamental challenge in robotics. This
paper presents Underwater Robot Self-Organizing Autonomy (UROSA), a
groundbreaking architecture leveraging distributed Large Language Model AI
agents integrated within the Robot Operating System 2 (ROS 2) framework to
enable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA
decentralises cognition into specialised AI agents responsible for multimodal
perception, adaptive reasoning, dynamic mission planning, and real-time
decision-making. Central innovations include flexible agents dynamically
adapting their roles, retrieval-augmented generation utilising vector databases
for efficient knowledge management, reinforcement learning-driven behavioural
optimisation, and autonomous on-the-fly ROS 2 node generation for runtime
functional extensibility. Extensive empirical validation demonstrates UROSA's
promising adaptability and reliability through realistic underwater missions in
simulation and real-world deployments, showing significant advantages over
traditional rule-based architectures in handling unforeseen scenarios,
environmental uncertainties, and novel mission objectives. This work not only
advances underwater autonomy but also establishes a scalable, safe, and
versatile cognitive robotics framework capable of generalising to a diverse
array of real-world applications.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Yvan R. Petillot</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23779v1" target="_blank">Phi-Ground Tech Report: Advancing Perception in GUI Grounding</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">æå‡GUIæ¥åœ°æ¨¡å‹åœ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, Baining Guo</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23751v1" target="_blank">CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">We propose CoT-Self-Instruct, a synthetic data generation method that
instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the
given seed tasks, and then to generate a new synthetic prompt of similar
quality and complexity for use in LLM training, followed by filtering for
high-quality data with automatic metrics. In verifiable reasoning, our
synthetic data significantly outperforms existing training datasets, such as
s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For
non-verifiable instruction-following tasks, our method surpasses the
performance of human or standard self-instruct prompts on both AlpacaEval 2.0
and Arena-Hard.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Ping Yu, Jack Lanchantin, Tianlu Wang, Weizhe Yuan, Olga Golovneva, Ilia Kulikov, Sainbayar Sukhbaatar, Jason Weston, Jing Xu</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23698v1" target="_blank">Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">While Reinforcement Learning (RL) has achieved remarkable success in language
modeling, its triumph hasn't yet fully translated to visuomotor agents. A
primary challenge in RL models is their tendency to overfit specific tasks or
environments, thereby hindering the acquisition of generalizable behaviors
across diverse settings. This paper provides a preliminary answer to this
challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can
achieve zero-shot generalization to unseen worlds. Specifically, we explore
RL's potential to enhance generalizable spatial reasoning and interaction
capabilities in 3D worlds. To address challenges in multi-task RL
representation, we analyze and establish cross-view goal specification as a
unified multi-task goal space for visuomotor policies. Furthermore, to overcome
the significant bottleneck of manual task design, we propose automated task
synthesis within the highly customizable Minecraft environment for large-scale
multi-task RL training, and we construct an efficient distributed RL framework
to support this. Experimental results show RL significantly boosts interaction
success rates by $4\times$ and enables zero-shot generalization of spatial
reasoning across diverse environments, including real-world settings. Our
findings underscore the immense potential of RL training in 3D simulated
environments, especially those amenable to large-scale task generation, for
significantly advancing visuomotor agents' spatial reasoning.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23771v1" target="_blank">Consensus-Driven Active Model Selection</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">The widespread availability of off-the-shelf machine learning models poses a
challenge: which model, of the many available candidates, should be chosen for
a given data analysis task? This question of model selection is traditionally
answered by collecting and annotating a validation dataset -- a costly and
time-intensive process. We propose a method for active model selection, using
predictions from candidate models to prioritize the labeling of test data
points that efficiently differentiate the best candidate. Our method, CODA,
performs consensus-driven active model selection by modeling relationships
between classifiers, categories, and data points within a probabilistic
framework. The framework uses the consensus and disagreement between models in
the candidate pool to guide the label acquisition process, and Bayesian
inference to update beliefs about which model is best as more information is
collected. We validate our approach by curating a collection of 26 benchmark
tasks capturing a range of model selection scenarios. CODA outperforms existing
methods for active model selection significantly, reducing the annotation
effort required to discover the best model by upwards of 70% compared to the
previous state-of-the-art. Code and data are available at
https://github.com/justinkay/coda.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Justin Kay, Grant Van Horn, Subhransu Maji, Daniel Sheldon, Sara Beery</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23704v1" target="_blank">Enhanced Velocity Field Modeling for Gaussian Video Reconstruction</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">High-fidelity 3D video reconstruction is essential for enabling real-time
rendering of dynamic scenes with realistic motion in virtual and augmented
reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has
achieved near-photorealistic results in video reconstruction due to the great
representation capability of deep deformation networks. However, in videos with
complex motion and significant scale variations, deformation networks often
overfit to irregular Gaussian trajectories, leading to suboptimal visual
quality. Moreover, the gradient-based densification strategy designed for
static scene reconstruction proves inadequate to address the absence of dynamic
content. In light of these challenges, we propose a flow-empowered velocity
field modeling scheme tailored for Gaussian video reconstruction, dubbed
FlowGaussian-VR. It consists of two core components: a velocity field rendering
(VFR) pipeline which enables optical flow-based optimization, and a
flow-assisted adaptive densification (FAD) strategy that adjusts the number and
size of Gaussians in dynamic regions. We validate our model's effectiveness on
multi-view dynamic reconstruction and novel view synthesis with multiple
real-world datasets containing challenging motion scenarios, demonstrating not
only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry
artifacts in dynamic textures, but also regularized and trackable per-Gaussian
trajectories.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Zhenyang Li, Xiaoyang Bai, Tongchen Zhang, Pengfei Shen, Weiwei Xu, Yifan Peng</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23740v1" target="_blank">Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Knowledge graphs (KGs) often contain sufficient information to support the
inference of new facts. Identifying logical rules not only improves the
completeness of a knowledge graph but also enables the detection of potential
errors, reveals subtle data patterns, and enhances the overall capacity for
reasoning and interpretation. However, the complexity of such rules, combined
with the unique labeling conventions of each KG, can make them difficult for
humans to understand. In this paper, we explore the potential of large language
models to generate natural language explanations for logical rules.
Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery
algorithm from the benchmark dataset FB15k-237 and two large-scale datasets,
FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including
zero- and few-shot prompting, including variable entity types, and
chain-of-thought reasoning. We conduct a comprehensive human evaluation of the
generated explanations based on correctness, clarity, and hallucination, and
also assess the use of large language models as automatic judges. Our results
demonstrate promising performance in terms of explanation correctness and
clarity, although several challenges remain for future research. All scripts
and data used in this study are publicly available at
https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Nasim Shirvani-Mahdavi, Devin Wingfield, Amin Ghasemi, Chengkai Li</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23701v1" target="_blank">TextQuests: How Good are LLMs at Text-Based Video Games?</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Evaluating AI agents within complex, interactive environments that mirror
real-world challenges is critical for understanding their practical
capabilities. While existing agent benchmarks effectively assess skills like
tool use or performance on structured tasks, they often do not fully capture an
agent's ability to operate autonomously in exploratory environments that demand
sustained, self-directed reasoning over a long and growing context. To spur the
development of agents capable of more robust intrinsic reasoning over long
horizons, we introduce TextQuests, a benchmark based on the Infocom suite of
interactive fiction games. These text-based adventures, which can take human
players over 30 hours and require hundreds of precise actions to solve, serve
as an effective proxy for evaluating AI agents on focused, stateful tasks. The
benchmark is specifically designed to assess an LLM agent's capacity for
self-contained problem-solving by precluding the use of external tools, thereby
focusing on intrinsic long-context reasoning capabilities in an exploratory
environment characterized by the need for trial-and-error learning and
sustained problem-solving within a single interactive session. We release
TextQuests at https://textquests.ai.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Long Phan, Mantas Mazeika, Andy Zou, Dan Hendrycks</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.23694v1" target="_blank">A survey of multi-agent geosimulation methodologies: from ABM to LLM</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 4</span>
          </div>
          <div class="summary-text">We provide a comprehensive examination of agent-based approaches that codify
the principles and linkages underlying multi-agent systems, simulations, and
information systems. Based on two decades of study, this paper confirms a
framework intended as a formal specification for geosimulation platforms. Our
findings show that large language models (LLMs) can be effectively incorporated
as agent components if they follow a structured architecture specific to
fundamental agent activities such as perception, memory, planning, and action.
This integration is precisely consistent with the architecture that we
formalize, providing a solid platform for next-generation geosimulation
systems.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Virginia Padilla, Jacinto DÃ¡vila</div>
        </div>
      
    </div>

    <div class="history-section">
        <h2>ğŸ“… å†å²æ—¥æŠ¥ç›®å½•</h2>
        <ul class="history-list">
          <li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-01.html" target="_blank">2025-08-01</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-31.html" target="_blank">2025-07-31</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-30.html" target="_blank">2025-07-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-29.html" target="_blank">2025-07-29</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-27.html" target="_blank">2025-07-27</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-26.html" target="_blank">2025-07-26</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-25.html" target="_blank">2025-07-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-24.html" target="_blank">2025-07-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-23.html" target="_blank">2025-07-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-22.html" target="_blank">2025-07-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-21.html" target="_blank">2025-07-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-20.html" target="_blank">2025-07-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-19.html" target="_blank">2025-07-19</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-18.html" target="_blank">2025-07-18</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-17.html" target="_blank">2025-07-17</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-16.html" target="_blank">2025-07-16</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-15.html" target="_blank">2025-07-15</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-13.html" target="_blank">2025-07-13</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-12.html" target="_blank">2025-07-12</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-11.html" target="_blank">2025-07-11</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-10.html" target="_blank">2025-07-10</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-09.html" target="_blank">2025-07-09</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-08.html" target="_blank">2025-07-08</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-07.html" target="_blank">2025-07-07</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-06.html" target="_blank">2025-07-06</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-05.html" target="_blank">2025-07-05</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-04.html" target="_blank">2025-07-04</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-03.html" target="_blank">2025-07-03</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-02.html" target="_blank">2025-07-02</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-01.html" target="_blank">2025-07-01</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-30.html" target="_blank">2025-06-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-29.html" target="_blank">2025-06-29</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-28.html" target="_blank">2025-06-28</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-27.html" target="_blank">2025-06-27</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-26.html" target="_blank">2025-06-26</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-25.html" target="_blank">2025-06-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-24.html" target="_blank">2025-06-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-23.html" target="_blank">2025-06-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-22.html" target="_blank">2025-06-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-21.html" target="_blank">2025-06-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-20.html" target="_blank">2025-06-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-19.html" target="_blank">2025-06-19</a></li>
        </ul>
      </div>

    <div class="footer">
      ğŸ”„ ç”± Cloudflare Workers + DeepSeek è‡ªåŠ¨ç”Ÿæˆ | æ›´æ–°æ—¶é—´: 2025/8/2 22:03:40
    </div>
  </div>
</body>
</html>