<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AIèµ„è®¯æ—¥æŠ¥ - 2025/7/8</title>
  <style>
    /* æ·»åŠ æŒ‰é’®æ ·å¼ */
    .nav-buttons {
      display: flex;
      gap: 15px;
      justify-content: center;
      margin-top: 15px;
      flex-wrap: wrap;
    }
    .nav-button {
      background: rgba(255, 255, 255, 0.2);
      border: 1px solid rgba(255, 255, 255, 0.4);
      border-radius: 20px;
      padding: 6px 15px;
      color: white;
      text-decoration: none;
      display: inline-flex;
      align-items: center;
      transition: all 0.3s;
      font-size: 0.9em;
    }
    .nav-button:hover {
      background: rgba(255, 255, 255, 0.3);
      transform: translateY(-2px);
    }
    /* å…¶ä»–æ ·å¼ä¿æŒä¸å˜ */
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: #f5f5f5; color: #333; margin: 0; padding: 0; }
    .container { max-width: 1200px; margin: auto; padding: 20px; }
    .header { background: linear-gradient(135deg, #667eea, #764ba2); color: #fff; padding: 40px 20px; border-radius: 10px; text-align: center; }
    .header h1 { font-size: 2.5em; margin: 0; }
    .summary, .history-section { background: #fff; margin-top: 30px; padding: 25px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
    .summary h2, .history-section h2 { color: #667eea; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px; }
    .news-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin-top: 30px; }
    .news-item { background: white; border-radius: 10px; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
    .news-item h3 a { text-decoration: none; color: #333; font-size: 1.1em; }
    .news-item h3 a:hover { color: #667eea; }
    .news-meta { font-size: 0.9em; color: #666; margin-bottom: 10px; }
    .category { display: inline-block; padding: 4px 10px; border-radius: 20px; font-size: 0.8em; margin-right: 8px; }
    .category.industry { background: #e3f2fd; color: #1976d2; }
    .category.academic { background: #f3e5f5; color: #7b1fa2; }
    .category.opensource { background: #e8f5e9; color: #388e3c; }
    .summary-text { margin-top: 10px; color: #555; }
    .importance { background: #ff9800; color: white; padding: 2px 8px; border-radius: 10px; font-size: 0.75em; margin-left: 10px; }
    .history-list { list-style: none; padding-left: 0; }
    .history-list li { margin: 5px 0; }
    .history-list a { text-decoration: none; color: #333; }
    .history-list a:hover { color: #667eea; }
    .footer { text-align: center; font-size: 0.9em; color: #999; padding: 20px; margin-top: 40px; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>ğŸ¤– AIèµ„è®¯æ—¥æŠ¥</h1>
      <p>2025/7/8 | äººå·¥æ™ºèƒ½é¢†åŸŸæœ€æ–°åŠ¨æ€</p>
      <div class="nav-buttons">
        <a href="../daily/2025-07-07.html" class="nav-button">
          ğŸ”™ æŸ¥çœ‹æ˜¨æ—¥å†…å®¹
        </a>
        <a href="../" class="nav-button">
          ğŸ  è¿”å›ä¸»é¡µ
        </a>
      </div>
    </div>

    <div class="summary">
      <h2>ğŸ“Š ä»Šæ—¥è¶‹åŠ¿æ€»ç»“</h2>
      <p>AIé¢†åŸŸæŒç»­å¿«é€Ÿå‘å±•ï¼Œæ¶µç›–äº†ä»ç†è®ºç ”ç©¶åˆ°å®é™…åº”ç”¨çš„å¹¿æ³›è¯é¢˜ã€‚å½“å‰è¶‹åŠ¿æ˜¾ç¤ºï¼Œè¡Œä¸šå†…éƒ¨å¯¹AIç®—æ³•çš„å®é™…åº”ç”¨ç—›ç‚¹ã€æŠ€æœ¯è¿›æ­¥é€Ÿåº¦ã€ä»¥åŠç›¸å…³æ³•å¾‹æ³•è§„çš„å…³æ³¨æ—¥ç›Šå¢åŠ ã€‚åŒæ—¶ï¼ŒAIæ•™è‚²å’ŒèŒä¸šæœºä¼šï¼Œå¦‚å®ä¹ å’Œç ”ç©¶ç”Ÿæ•™è‚²ï¼Œä¹Ÿæˆä¸ºäº†çƒ­é—¨è¯é¢˜ã€‚æ­¤å¤–ï¼ŒAIåœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ç‰¹å®šé¢†åŸŸçš„åº”ç”¨å’Œæˆæœ¬æ•ˆç›Šé«˜çš„è®¡ç®—èµ„æºä¹Ÿå—åˆ°äº†å…³æ³¨ã€‚</p>
    </div>

    <div class="news-grid">
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=36233487" target="_blank">Ask HN: Is the rate of progress in AI exponential?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">æ¢è®¨AIè¿›æ­¥é€Ÿåº¦æ˜¯å¦å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=27111719" target="_blank">Ask HN: What's the pain using current AI algorithms?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">æ¢è®¨å½“å‰AIç®—æ³•çš„ä½¿ç”¨ç—›ç‚¹ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://lekta.ai/blog/natural-language-processing-artificial-intelligence-machine-learning-bots-a-passing-trend-or-much-more" target="_blank">NLP, AI, ML, bots â€“ a passing trend or much more? What's your take on this?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">è®¨è®ºNLPã€AIã€MLå’Œæœºå™¨äººæ˜¯çŸ­æš‚è¶‹åŠ¿è¿˜æ˜¯æ›´æ·±è¿œçš„å˜é©ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=401541" target="_blank">Common Lisp + Machine Learning Internship at Google (Mountain View, CA)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">è°·æ­Œæä¾›Common Lispä¸æœºå™¨å­¦ä¹ å®ä¹ æœºä¼šã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=12995049" target="_blank">Ask HN: Dipping my toes with artificial intelligence and what to expect? (CS)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">åˆå­¦è€…æ¢è®¨æ¶‰è¶³AIé¢†åŸŸçš„é¢„æœŸã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=18049136" target="_blank">50% Cheaper GPUs for cloud-computing / Saving devs 50% compared to AWS</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">æä¾›æ¯”AWSä¾¿å®œ50%çš„GPUäº‘è®¡ç®—èµ„æºã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=35103021" target="_blank">The AI Crackpot Index</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">AI CrackpotæŒ‡æ•°ï¼Œæ¢è®¨AIé¢†åŸŸçš„éä¸»æµè§‚ç‚¹ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://www.yourobot.io/blog/uncategorized/the-next-bill-gates-or-albert-einstein-in-ai-artificial-intelligence-will-produce-the-god-algorithm-of-machine-learning-where-a-machine-will-be-able-to-do-and-learn-anything-by-its-self/" target="_blank">The Next Bill Gates or Albert Einstein in AI â€œChris Clarkâ€ â€“ Yourobot</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">æ¢è®¨AIé¢†åŸŸçš„ä¸‹ä¸€ä¸ªæ¯”å°”Â·ç›–èŒ¨æˆ–çˆ±å› æ–¯å¦ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=244100" target="_blank">Ask HN: Thoughts on grad school? (CS PhD)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">æ¢è®¨è®¡ç®—æœºç§‘å­¦ç ”ç©¶ç”Ÿæ•™è‚²çš„çœ‹æ³•ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=36431356" target="_blank">Ask HN: Anyone concerned about NYC Local Law 144?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 4</span>
          </div>
          <div class="summary-text">è®¨è®ºå¯¹çº½çº¦å¸‚åœ°æ–¹æ³•å¾‹144å·çš„å…³æ³¨ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=15140715" target="_blank">Bioinformatician</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 4</span>
          </div>
          <div class="summary-text">ç”Ÿç‰©ä¿¡æ¯å­¦å®¶çš„èŒä¸šæœºä¼šã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=42619160" target="_blank">Show HN: Startup Raising capital through Book Sales</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 3</span>
          </div>
          <div class="summary-text">åˆåˆ›å…¬å¸é€šè¿‡ä¹¦ç±é”€å”®ç­¹é›†èµ„é‡‘ã€‚</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/Lightning-AI/litgpt" target="_blank">Lightning-AI/litgpt</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 10</span>
          </div>
          <div class="summary-text">20+é«˜æ€§èƒ½LLMsåŠé¢„è®­ç»ƒã€å¾®è°ƒå’Œéƒ¨ç½²æ–¹æ¡ˆ</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12452 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/carla-simulator/carla" target="_blank">carla-simulator/carla</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">è‡ªåŠ¨é©¾é©¶ç ”ç©¶çš„å¼€æºæ¨¡æ‹Ÿå™¨</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12693 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/extreme-assistant/CVPR2024-Paper-Code-Interpretation" target="_blank">extreme-assistant/CVPR2024-Paper-Code-Interpretation</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">CVPRè®ºæ–‡/ä»£ç /è§£è¯»åˆé›†</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12500 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/DataTalksClub/mlops-zoomcamp" target="_blank">DataTalksClub/mlops-zoomcamp</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">DataTalks.Clubæä¾›çš„å…è´¹MLOpsè¯¾ç¨‹</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12966 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/alphacep/vosk-api" target="_blank">alphacep/vosk-api</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">æ”¯æŒAndroidã€iOSç­‰çš„ç¦»çº¿è¯­éŸ³è¯†åˆ«API</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12636 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/jina-ai/clip-as-service" target="_blank">jina-ai/clip-as-service</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">ä½¿ç”¨CLIPè¿›è¡Œå›¾åƒå’Œå¥å­çš„å¯æ‰©å±•åµŒå…¥ã€æ¨ç†å’Œæ’å</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12697 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/junyanz/CycleGAN" target="_blank">junyanz/CycleGAN</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">èƒ½å¤Ÿå°†ç…§ç‰‡è½¬æ¢ä¸ºç»˜ç”»ã€é©¬å˜æ–‘é©¬ç­‰çš„è½¯ä»¶</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12690 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/chenyuntc/pytorch-book" target="_blank">chenyuntc/pytorch-book</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">PyTorchæ•™ç¨‹å’Œæœ‰è¶£é¡¹ç›®ï¼ŒåŒ…æ‹¬ç¥ç»å¯¹è¯å’Œé£æ ¼è½¬æ¢</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12536 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/apache/predictionio" target="_blank">apache/predictionio</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">é¢å‘å¼€å‘è€…å’ŒMLå·¥ç¨‹å¸ˆçš„æœºå™¨å­¦ä¹ æœåŠ¡å™¨</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12528 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/isl-org/Open3D" target="_blank">isl-org/Open3D</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">ç°ä»£3Dæ•°æ®å¤„ç†åº“</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12521 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/ggml-org/ggml" target="_blank">ggml-org/ggml</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">æœºå™¨å­¦ä¹ å¼ é‡åº“</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12793 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://github.com/kmario23/deep-learning-drizzle" target="_blank">kmario23/deep-learning-drizzle</a></h3>
          <div class="news-meta">
            <span class="category opensource">å¼€æºé¡¹ç›®</span>
            <span>GitHub</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ç­‰é¢†åŸŸçš„ç²¾é€‰èµ„æº</div>
          <div style="margin-top:5px; font-size:0.9em; color:#666;">â­ 12607 stars</div>
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05257v1" target="_blank">Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">æå‡ºMemoryAgentBenchï¼Œè¯„ä¼°LLMä»£ç†çš„è®°å¿†èƒ½åŠ›ï¼Œæ¶µç›–å››ä¸ªæ ¸å¿ƒèƒ½åŠ›ã€‚</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Yuanzhe Hu, Yu Wang, Julian McAuley</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05241v1" target="_blank">SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">The rapid advancements of AI agents have ignited the long-held ambition of
leveraging them to accelerate scientific discovery. Achieving this goal
requires a deep understanding of the frontiers of human knowledge. As such,
Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for
evaluating scientific AI agents. In this work, we aim to construct the
foundational architecture for general-purpose agents and validate the
capabilities through leading performance on HLE. To achieve this, we introduce
X-Master, a tool-augmented reasoning agent designed to emulate human
researchers by interacting flexibly with external tools during its reasoning
process. This agent, guided by the conceptualization of code as an interaction
language, can flexibly leverage built-in Python libraries and our customized
tools to augment the reasoning. We further scale its capabilities through
X-Masters, a scattered-and-stacked agentic workflow that systematically
enhances breadth and depth of reasoning. Our open-source solution, X-Masters,
sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing
OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to
exceed the 30% threshold. This work allows us to gain a deeper understanding of
complex task-solving and accumulates valuable experience that can inform future
advancements, guiding subsequent model training.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Weinan E, Siheng Chen</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05254v1" target="_blank">From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">ç³»ç»Ÿç ”ç©¶è”åˆè¿åŠ¨é¢„æµ‹æ–¹æ³•ï¼Œè¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§ã€å¤šæ¨¡æ€æ€§å’Œæ¨ç†æ•ˆç‡ã€‚</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Fabian Konstantinidis, Ariel Dallari Guerreiro, Raphael Trumpp, Moritz Sackmann, Ulrich Hofmann, Marco Caccamo, Christoph Stiller</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05251v1" target="_blank">Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">ä»‹ç»ä¸¤ç§åŠ¨ä½œç©ºé—´ç¼©å‡ç­–ç•¥ï¼Œæé«˜è‡ªåŠ¨é©¾é©¶ä¸­å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ•ˆç‡å’Œç­–ç•¥æ€§èƒ½ã€‚</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Elahe Delavari, Feeza Khan Khanzada, Jaerock Kwon</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05211v1" target="_blank">All in One: Visual-Description-Guided Unified Point Cloud Segmentation</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">Unified segmentation of 3D point clouds is crucial for scene understanding,
but is hindered by its sparse structure, limited annotations, and the challenge
of distinguishing fine-grained object classes in complex environments. Existing
methods often struggle to capture rich semantic and contextual information due
to limited supervision and a lack of diverse multimodal cues, leading to
suboptimal differentiation of classes and instances. To address these
challenges, we propose VDG-Uni3DSeg, a novel framework that integrates
pre-trained vision-language models (e.g., CLIP) and large language models
(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual
descriptions and reference images from the internet, our method incorporates
rich multimodal cues, facilitating fine-grained class and instance separation.
We further design a Semantic-Visual Contrastive Loss to align point features
with multimodal queries and a Spatial Enhanced Module to model scene-wide
relationships efficiently. Operating within a closed-set paradigm that utilizes
multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art
results in semantic, instance, and panoptic segmentation, offering a scalable
and practical solution for 3D understanding. Our code is available at
https://github.com/Hanzy1996/VDG-Uni3DSeg.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05201v1" target="_blank">MedGemma Technical Report</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">Artificial intelligence (AI) has significant potential in healthcare
applications, but its training and deployment faces challenges due to
healthcare's diverse data, complex tasks, and the need to preserve privacy.
Foundation models that perform well on medical tasks and require less
task-specific tuning data are critical to accelerate the development of
healthcare AI applications. We introduce MedGemma, a collection of medical
vision-language foundation models based on Gemma 3 4B and 27B. MedGemma
demonstrates advanced medical understanding and reasoning on images and text,
significantly exceeding the performance of similar-sized generative models and
approaching the performance of task-specific models, while maintaining the
general capabilities of the Gemma 3 base models. For out-of-distribution tasks,
MedGemma achieves 2.6-10% improvement on medical multimodal question answering,
15.5-18.1% improvement on chest X-ray finding classification, and 10.8%
improvement on agentic evaluations compared to the base models. Fine-tuning
MedGemma further improves performance in subdomains, reducing errors in
electronic health record information retrieval by 50% and reaching comparable
performance to existing specialized state-of-the-art methods for pneumothorax
classification and histopathology patch classification. We additionally
introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.
MedSigLIP powers the visual understanding capabilities of MedGemma and as an
encoder achieves comparable or better performance than specialized medical
image encoders. Taken together, the MedGemma collection provides a strong
foundation of medical image and text capabilities, with potential to
significantly accelerate medical research and development of downstream
applications. The MedGemma collection, including tutorials and model weights,
can be found at https://goo.gle/medgemma.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, CÃ­an Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry, Lepikhin, Vlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, LÃ©onard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, Lin Yang</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05198v1" target="_blank">EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">The rapid advancement of Embodied AI has led to an increasing demand for
large-scale, high-quality real-world data. However, collecting such embodied
data remains costly and inefficient. As a result, simulation environments have
become a crucial surrogate for training robot policies. Yet, the significant
Real2Sim2Real gap remains a critical bottleneck, particularly in terms of
physical dynamics and visual appearance. To address this challenge, we propose
EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both
the physics and appearance perspectives. Specifically, we propose PhysAligner,
a differentiable physics module designed to reduce the Real2Sim physical gap.
It jointly optimizes robot-specific parameters such as control gains and
friction coefficients to better align simulated dynamics with real-world
observations. In addition, we introduce VisAligner, which incorporates a
conditional video diffusion model to bridge the Sim2Real appearance gap by
translating low-fidelity simulated renderings into photorealistic videos
conditioned on simulation states, enabling high-fidelity visual transfer.
Extensive experiments validate the effectiveness of EmbodieDreamer. The
proposed PhysAligner reduces physical parameter estimation error by 3.74%
compared to simulated annealing methods while improving optimization speed by
89.91\%. Moreover, training robot policies in the generated photorealistic
environment leads to a 29.17% improvement in the average task success rate
across real-world tasks after reinforcement learning. Code, model and data will
be publicly available.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, Xingang Wang</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05187v1" target="_blank">Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">The proliferation of AI-driven systems presents a fundamental challenge to
Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work
(CSCW), often diminishing user agency and failing to account for value
pluralism. Current approaches to value alignment, which rely on centralized,
top-down definitions, lack the mechanisms for meaningful contestability. This
leaves users and communities unable to challenge or shape the values embedded
in the systems that govern their digital lives, creating a crisis of legitimacy
and trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP),
a socio-technical framework that addresses this gap. It reframes the design
problem from achieving a single aligned state to infrastructuring a dynamic
ecosystem for value deliberation and application. At its core, CDAVP enables
diverse, self-organizing communities to define and maintain explicit value
profiles - rich, machine-readable representations that can encompass not only
preferences but also community-specific rights and duties. These profiles are
then contextually activated by the end-user, who retains ultimate control
(agency) over which values guide the AI's behavior. AI applications, in turn,
are designed to transparently interpret these profiles and moderate conflicts,
adhering to a set of non-negotiable, democratically-legitimated meta-rules. The
designer's role shifts from crafting static interfaces to becoming an architect
of participatory ecosystems. We argue that infrastructuring for pluralism is a
necessary pathway toward achieving robust algorithmic accountability and
genuinely contestable, human-centric AI.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Andreas Mayer</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05246v1" target="_blank">When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">While chain-of-thought (CoT) monitoring is an appealing AI safety defense,
recent work on "unfaithfulness" has cast doubt on its reliability. These
findings highlight an important failure mode, particularly when CoT acts as a
post-hoc rationalization in applications like auditing for bias. However, for
the distinct problem of runtime monitoring to prevent severe harm, we argue the
key property is not faithfulness but monitorability. To this end, we introduce
a conceptual framework distinguishing CoT-as-rationalization from
CoT-as-computation. We expect that certain classes of severe harm will require
complex, multi-step reasoning that necessitates CoT-as-computation. Replicating
the experimental setups of prior work, we increase the difficulty of the bad
behavior to enforce this necessity condition; this forces the model to expose
its reasoning, making it monitorable. We then present methodology guidelines to
stress-test CoT monitoring against deliberate evasion. Applying these
guidelines, we find that models can learn to obscure their intentions, but only
when given significant help, such as detailed human-written strategies or
iterative optimization against the monitor. We conclude that, while not
infallible, CoT monitoring offers a substantial layer of defense that requires
active protection and continued stress-testing.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, Rohin Shah</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05244v1" target="_blank">Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">In collaborative tasks, being able to adapt to your teammates is a necessary
requirement for success. When teammates are heterogeneous, such as in
human-agent teams, agents need to be able to observe, recognize, and adapt to
their human partners in real time. This becomes particularly challenging in
tasks with time pressure and complex strategic spaces where the dynamics can
change rapidly. In this work, we introduce TALENTS, a strategy-conditioned
cooperator framework that learns to represent, categorize, and adapt to a range
of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a
variational autoencoder to learn a latent strategy space from trajectory data.
This latent space represents the underlying strategies that agents employ.
Subsequently, the system identifies different types of strategy by clustering
the data. Finally, a cooperator agent is trained to generate partners for each
type of strategy, conditioned on these clusters. In order to adapt to
previously unseen partners, we leverage a fixed-share regret minimization
algorithm that infers and adjusts the estimated partner strategy dynamically.
We assess our approach in a customized version of the Overcooked environment,
posing a challenging cooperative cooking task that demands strong coordination
across a wide range of possible strategies. Using an online user study, we show
that our agent outperforms current baselines when working with unfamiliar human
partners.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Benjamin Li, Shuyang Shi, Lucia Romero, Huao Li, Yaqi Xie, Woojun Kim, Stefanos Nikolaidis, Michael Lewis, Katia Sycara, Simon Stepputtis</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05221v1" target="_blank">CTA: Cross-Task Alignment for Better Test Time Training</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">Deep learning models have demonstrated exceptional performance across a wide
range of computer vision tasks. However, their performance often degrades
significantly when faced with distribution shifts, such as domain or dataset
changes. Test-Time Training (TTT) has emerged as an effective method to enhance
model robustness by incorporating an auxiliary unsupervised task during
training and leveraging it for model updates at test time. In this work, we
introduce CTA (Cross-Task Alignment), a novel approach for improving TTT.
Unlike existing TTT methods, CTA does not require a specialized model
architecture and instead takes inspiration from the success of multi-modal
contrastive learning to align a supervised encoder with a self-supervised one.
This process enforces alignment between the learned representations of both
models, thereby mitigating the risk of gradient interference, preserving the
intrinsic robustness of self-supervised learning and enabling more semantically
meaningful updates at test-time. Experimental results demonstrate substantial
improvements in robustness and generalization over the state-of-the-art on
several benchmark datasets.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Samuel Barbeau, Pedram Fekri, David Osowiechi, Ali Bahri, Moslem YazdanpanahMasih Aminbeidokhti, Christian Desrosiers</div>
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2507.05195v1" target="_blank">Train-before-Test Harmonizes Language Model Rankings</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">Existing language model benchmarks provide contradictory model rankings, even
for benchmarks that aim to capture similar skills. This dilemma of conflicting
rankings hampers model selection, clouds model comparisons, and adds confusion
to a growing ecosystem of competing models. Recent work attributed ranking
disagreement to the phenomenon of training on the test task: As released,
different models exhibit a different level of preparation for any given test
task. A candidate solution to the problem is train-before-test: Give each model
the same benchmark-specific finetuning before evaluation. Our primary
contribution is a broad empirical evaluation of train-before-test across 24
benchmarks and 61 models. We show that train-before-test significantly improves
ranking agreement consistently across all benchmarks. Whereas rankings have
little external validity to start with, they enjoy a significant degree of
external validity when applying train-before-test: Model rankings transfer
gracefully from one benchmark to the other. Even within the same model family,
train-before-test reduces strong ranking disagreement to near-perfect
agreement. In addition, train-before-test reduces the model-score matrix to
essentially rank one, revealing new insights into the latent factors of
benchmark performance. Our work supports the recommendation to make
train-before-test a default component of LLM benchmarking.</div>
          
          <div style="margin-top:5px; font-size:0.9em; color:#666;">ğŸ‘¨â€ğŸ”¬ Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt</div>
        </div>
      
    </div>

    <div class="history-section">
        <h2>ğŸ“… å†å²æ—¥æŠ¥ç›®å½•</h2>
        <ul class="history-list">
          <li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-07.html" target="_blank">2025-07-07</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-06.html" target="_blank">2025-07-06</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-05.html" target="_blank">2025-07-05</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-04.html" target="_blank">2025-07-04</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-03.html" target="_blank">2025-07-03</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-02.html" target="_blank">2025-07-02</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-01.html" target="_blank">2025-07-01</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-30.html" target="_blank">2025-06-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-29.html" target="_blank">2025-06-29</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-28.html" target="_blank">2025-06-28</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-27.html" target="_blank">2025-06-27</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-26.html" target="_blank">2025-06-26</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-25.html" target="_blank">2025-06-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-24.html" target="_blank">2025-06-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-23.html" target="_blank">2025-06-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-22.html" target="_blank">2025-06-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-21.html" target="_blank">2025-06-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-20.html" target="_blank">2025-06-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-19.html" target="_blank">2025-06-19</a></li>
        </ul>
      </div>

    <div class="footer">
      ğŸ”„ ç”± Cloudflare Workers + DeepSeek è‡ªåŠ¨ç”Ÿæˆ | æ›´æ–°æ—¶é—´: 2025/7/8 22:04:26
    </div>
  </div>
</body>
</html>