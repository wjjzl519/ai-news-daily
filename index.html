<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI资讯日报 - 2025/6/18</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6; 
            color: #333; 
            background: #f5f5f5;
        }
        .container { 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 20px;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 20px;
            text-align: center;
            border-radius: 10px;
            margin-bottom: 30px;
        }
        .header h1 { font-size: 2.5em; margin-bottom: 10px; }
        .header p { font-size: 1.2em; opacity: 0.9; }
        .summary {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }
        .summary h2 { 
            color: #667eea; 
            margin-bottom: 15px;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }
        .news-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
        }
        .news-item {
            background: white;
            border-radius: 10px;
            padding: 25px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            transition: transform 0.2s;
        }
        .news-item:hover { transform: translateY(-5px); }
        .news-item h3 { 
            color: #333; 
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        .news-item h3 a {
            color: #333;
            text-decoration: none;
        }
        .news-item h3 a:hover { color: #667eea; }
        .news-meta {
            display: flex;
            align-items: center;
            gap: 15px;
            margin-bottom: 15px;
            font-size: 0.9em;
            color: #666;
        }
        .category {
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 500;
        }
        .category.industry { background: #e3f2fd; color: #1976d2; }
        .category.academic { background: #f3e5f5; color: #7b1fa2; }
        .category.opensource { background: #e8f5e8; color: #388e3c; }
        .summary-text { 
            color: #555; 
            line-height: 1.5;
            margin-bottom: 15px;
        }
        .importance {
            display: inline-block;
            background: #ff9800;
            color: white;
            padding: 2px 8px;
            border-radius: 10px;
            font-size: 0.8em;
            font-weight: bold;
        }
        .footer {
            text-align: center;
            margin-top: 50px;
            padding: 20px;
            color: #666;
        }
        @media (max-width: 768px) {
            .container { padding: 10px; }
            .header { padding: 20px; }
            .header h1 { font-size: 2em; }
            .news-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🤖 AI资讯日报</h1>
            <p>2025/6/18 | 人工智能领域最新动态</p>
        </div>
        
        <div class="summary">
            <h2>📊 今日趋势总结</h2>
            <p>今日AI资讯汇总（自动处理暂时不可用）</p>
        </div>
        
        <div class="news-grid">
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=35103021" target="_blank">The AI Crackpot Index</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">The AI Crackpot Index</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=27111719" target="_blank">Ask HN: What's the pain using current AI algorithms?</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Ask HN: What's the pain using current AI algorithms?</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=401541" target="_blank">Common Lisp + Machine Learning Internship at Google (Mountain View, CA)</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Common Lisp + Machine Learning Internship at Google (Mountain View, CA)</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=36431356" target="_blank">Ask HN: Anyone concerned about NYC Local Law 144?</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Ask HN: Anyone concerned about NYC Local Law 144?</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=36233487" target="_blank">Ask HN: Is the rate of progress in AI exponential?</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Ask HN: Is the rate of progress in AI exponential?</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=42619160" target="_blank">Show HN: Startup Raising capital through Book Sales</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Show HN: Startup Raising capital through Book Sales</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="http://www.yourobot.io/blog/uncategorized/the-next-bill-gates-or-albert-einstein-in-ai-artificial-intelligence-will-produce-the-god-algorithm-of-machine-learning-where-a-machine-will-be-able-to-do-and-learn-anything-by-its-self/" target="_blank">The Next Bill Gates or Albert Einstein in AI “Chris Clark” – Yourobot</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">The Next Bill Gates or Albert Einstein in AI “Chris Clark” – Yourobot</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=15140715" target="_blank">Bioinformatician</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Bioinformatician</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://lekta.ai/blog/natural-language-processing-artificial-intelligence-machine-learning-bots-a-passing-trend-or-much-more" target="_blank">NLP, AI, ML, bots – a passing trend or much more? What's your take on this?</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">NLP, AI, ML, bots – a passing trend or much more? What's your take on this?</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="https://news.ycombinator.com/item?id=12995049" target="_blank">Ask HN: Dipping my toes with artificial intelligence and what to expect? (CS)</a></h3>
                    <div class="news-meta">
                        <span class="category industry">行业动态</span>
                        <span>Hacker News</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Ask HN: Dipping my toes with artificial intelligence and what to expect? (CS)</div>
                    
                    
                </div>
            
                <div class="news-item">
                    <h3><a href="http://arxiv.org/abs/2506.14767v1" target="_blank">A Variational Framework for Improving Naturalness in Generative Spoken Language Models</a></h3>
                    <div class="news-meta">
                        <span class="category academic">学术论文</span>
                        <span>ArXiv</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">The success of large language models in text processing has inspired their
adaptation to speech modeling. However, since speech is continuous and complex,
it is often discretized for autoregressive modeling. Speech tokens derived from
self-supervised models (known as semantic tokens) typically focus on the
linguistic aspects of speech but neglect prosodic information. As a result,
models trained on these tokens can generate speech with reduced naturalness.
Existing approaches try to fix this by adding pitch features to the semantic
tokens. However, pitch alone cannot fully represent the range of paralinguistic
attributes, and selecting the right features requires careful hand-engineering.
To overcome this, we propose an end-to-end variational approach that
automatically learns to encode these continuous speech attributes to enhance
the semantic tokens. Our approach eliminates the need for manual extraction and
selection of paralinguistic features. Moreover, it produces preferred speech
continuations according to human raters. Code, samples and models are available
at https://github.com/b04901014/vae-gslm.</div>
                    
                    <div style="color: #666; font-size: 0.9em;">👨‍🔬 Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky</div>
                </div>
            
                <div class="news-item">
                    <h3><a href="http://arxiv.org/abs/2506.14761v1" target="_blank">From Bytes to Ideas: Language Modeling with Autoregressive U-Nets</a></h3>
                    <div class="news-meta">
                        <span class="category academic">学术论文</span>
                        <span>ArXiv</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Tokenization imposes a fixed granularity on the input text, freezing how a
language model operates on data and how far in the future it predicts. Byte
Pair Encoding (BPE) and similar schemes split text once, build a static
vocabulary, and leave the model stuck with that choice. We relax this rigidity
by introducing an autoregressive U-Net that learns to embed its own tokens as
it trains. The network reads raw bytes, pools them into words, then pairs of
words, then up to 4 words, giving it a multi-scale view of the sequence. At
deeper stages, the model must predict further into the future -- anticipating
the next few words rather than the next byte -- so deeper stages focus on
broader semantic patterns while earlier stages handle fine details. When
carefully tuning and controlling pretraining compute, shallow hierarchies tie
strong BPE baselines, and deeper hierarchies have a promising trend. Because
tokenization now lives inside the model, the same system can handle
character-level tasks and carry knowledge across low-resource languages.</div>
                    
                    <div style="color: #666; font-size: 0.9em;">👨‍🔬 Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, David Lopez-Paz</div>
                </div>
            
                <div class="news-item">
                    <h3><a href="http://arxiv.org/abs/2506.14755v1" target="_blank">Optimizing Length Compression in Large Reasoning Models</a></h3>
                    <div class="news-meta">
                        <span class="category academic">学术论文</span>
                        <span>ArXiv</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">Large Reasoning Models (LRMs) have achieved remarkable success, yet they
often suffer from producing unnecessary and verbose reasoning chains. We
identify a core aspect of this issue as "invalid thinking" -- models tend to
repeatedly double-check their work after having derived the correct answer. To
address this specific inefficiency, we move beyond the general principles of
Efficacy and Efficiency to propose two new, fine-grained principles: Brevity,
which advocates for eliminating redundancy, and Sufficiency, which ensures
critical reasoning steps are preserved. Guided by these principles, we
introduce LC-R1, a post-training method based on Group Relative Policy
Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for
overall conciseness and a Compress Reward that is specifically designed to
remove the invalid portion of the thinking process. Extensive experiments on
multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant
reduction in sequence length (~50%) with only a marginal (~2%) drop in
accuracy, achieving a favorable trade-off point on the Pareto frontier that
prioritizes high compression. Our analysis further validates the robustness of
LC-R1 and provides valuable insights for developing more powerful yet
computationally efficient LRMs. Our code is released at
https://github.com/zxiangx/LC-R1.</div>
                    
                    <div style="color: #666; font-size: 0.9em;">👨‍🔬 Zhengxiang Cheng, Dongping Chen, Mingyang Fu, Tianyi Zhou</div>
                </div>
            
                <div class="news-item">
                    <h3><a href="http://arxiv.org/abs/2506.14750v1" target="_blank">Exploring Speaker Diarization with Mixture of Experts</a></h3>
                    <div class="news-meta">
                        <span class="category academic">学术论文</span>
                        <span>ArXiv</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">In this paper, we propose a novel neural speaker diarization system using
memory-aware multi-speaker embedding with sequence-to-sequence architecture
(NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with
a sequence-to-sequence architecture. The system leverages a memory module to
enhance speaker embeddings and employs a Seq2Seq framework to efficiently map
acoustic features to speaker labels. Additionally, we explore the application
of mixture of experts in speaker diarization, and introduce a Shared and Soft
Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance
performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE.
Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo,
Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in
robustness and generalization. The proposed methods achieve state-of-the-art
results, showcasing their effectiveness in challenging real-world scenarios.</div>
                    
                    <div style="color: #666; font-size: 0.9em;">👨‍🔬 Gaobin Yang, Maokui He, Shutong Niu, Ruoyu Wang, Hang Chen, Jun Du</div>
                </div>
            
                <div class="news-item">
                    <h3><a href="http://arxiv.org/abs/2506.14731v1" target="_blank">Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs</a></h3>
                    <div class="news-meta">
                        <span class="category academic">学术论文</span>
                        <span>ArXiv</span>
                        <span class="importance">重要度: 5</span>
                    </div>
                    <div class="summary-text">We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model
optimized via reinforcement learning (RL) to achieve efficient and robust
reasoning capabilities. Built upon the publicly available Ling-lite model, a
16.8 billion parameter model with 2.75 billion activated parameters, our
approach matches the performance of state-of-the-art (SOTA) small-scale
reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,
GPQA-Diamond) while activating only one-third of the parameters required by
comparable models. To accomplish this, we introduce a joint training pipeline
integrating distillation with RL, revealing undocumented challenges in MoE RL
training. First, we identify optimization instability during RL training, and
we propose Constrained Contextual Computation Policy Optimization(C3PO), a
novel approach that enhances training stability and improves computational
throughput via algorithm-system co-design methodology. Second, we empirically
demonstrate that selecting distillation checkpoints based on entropy loss for
RL training, rather than validation metrics, yields superior
performance-efficiency trade-offs in subsequent RL training. Finally, we
develop a two-stage training paradigm to harmonize multi-domain data
integration, addressing domain conflicts that arise in training with mixed
dataset. We will release the model, dataset, and code.</div>
                    
                    <div style="color: #666; font-size: 0.9em;">👨‍🔬 Ring Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen</div>
                </div>
            
        </div>
        
        <div class="footer">
            <p>🔄 由 Cloudflare Workers + Google Gemini 自动生成</p>
            <p>📅 更新时间: 2025/6/18 10:29:27</p>
        </div>
    </div>
</body>
</html>