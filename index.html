<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AIèµ„è®¯æ—¥æŠ¥ - 2025/11/3</title>
  <style>
    /* æ·»åŠ æŒ‰é’®æ ·å¼ */
    .nav-buttons {
      display: flex;
      gap: 15px;
      justify-content: center;
      margin-top: 15px;
      flex-wrap: wrap;
    }
    .nav-button {
      background: rgba(255, 255, 255, 0.2);
      border: 1px solid rgba(255, 255, 255, 0.4);
      border-radius: 20px;
      padding: 6px 15px;
      color: white;
      text-decoration: none;
      display: inline-flex;
      align-items: center;
      transition: all 0.3s;
      font-size: 0.9em;
    }
    .nav-button:hover {
      background: rgba(255, 255, 255, 0.3);
      transform: translateY(-2px);
    }
    /* å…¶ä»–æ ·å¼ä¿æŒä¸å˜ */
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: #f5f5f5; color: #333; margin: 0; padding: 0; }
    .container { max-width: 1200px; margin: auto; padding: 20px; }
    .header { background: linear-gradient(135deg, #667eea, #764ba2); color: #fff; padding: 40px 20px; border-radius: 10px; text-align: center; }
    .header h1 { font-size: 2.5em; margin: 0; }
    .summary, .history-section { background: #fff; margin-top: 30px; padding: 25px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
    .summary h2, .history-section h2 { color: #667eea; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px; }
    .news-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin-top: 30px; }
    .news-item { background: white; border-radius: 10px; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
    .news-item h3 a { text-decoration: none; color: #333; font-size: 1.1em; }
    .news-item h3 a:hover { color: #667eea; }
    .news-meta { font-size: 0.9em; color: #666; margin-bottom: 10px; }
    .category { display: inline-block; padding: 4px 10px; border-radius: 20px; font-size: 0.8em; margin-right: 8px; }
    .category.industry { background: #e3f2fd; color: #1976d2; }
    .category.academic { background: #f3e5f5; color: #7b1fa2; }
    .category.opensource { background: #e8f5e9; color: #388e3c; }
    .summary-text { margin-top: 10px; color: #555; }
    .importance { background: #ff9800; color: white; padding: 2px 8px; border-radius: 10px; font-size: 0.75em; margin-left: 10px; }
    .history-list { list-style: none; padding-left: 0; }
    .history-list li { margin: 5px 0; }
    .history-list a { text-decoration: none; color: #333; }
    .history-list a:hover { color: #667eea; }
    .footer { text-align: center; font-size: 0.9em; color: #999; padding: 20px; margin-top: 40px; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>ğŸ¤– AIèµ„è®¯æ—¥æŠ¥</h1>
      <p>2025/11/3 | äººå·¥æ™ºèƒ½é¢†åŸŸæœ€æ–°åŠ¨æ€</p>
      <div class="nav-buttons">
        <a href="../daily/2025-11-02.html" class="nav-button">
          ğŸ”™ æŸ¥çœ‹æ˜¨æ—¥å†…å®¹
        </a>
        <a href="../" class="nav-button">
          ğŸ  è¿”å›ä¸»é¡µ
        </a>
      </div>
    </div>

    <div class="summary">
      <h2>ğŸ“Š ä»Šæ—¥è¶‹åŠ¿æ€»ç»“</h2>
      <p>ä»è¿™äº›Hacker Newsèµ„è®¯å¯ä»¥çœ‹å‡ºAIè¡Œä¸šå‘ˆç°å¤šå…ƒåŒ–å‘å±•æ€åŠ¿ï¼šä¸€æ–¹é¢ï¼Œä¸šç•ŒæŒç»­å…³æ³¨AIæŠ€æœ¯çš„å®é™…åº”ç”¨ç—›ç‚¹ã€å‘å±•é€Ÿåº¦è¯„ä¼°ä»¥åŠé•¿æœŸè¶‹åŠ¿åˆ¤æ–­ï¼›å¦ä¸€æ–¹é¢ï¼Œç¤¾åŒºå¯¹AIå­¦ä¹ èµ„æºã€èŒä¸šæœºä¼šå’Œç›‘ç®¡æ”¿ç­–è¡¨ç°å‡ºæµ“åšå…´è¶£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ—¢æœ‰å¯¹AIè¿‡åº¦ç‚’ä½œçš„åæ€ï¼ˆå¦‚AI Crackpot Indexï¼‰ï¼Œä¹Ÿæœ‰å¯¹åŸºç¡€ä¸šåŠ¡æ¨¡å¼æŒä¹…æ€§çš„è®¨è®ºï¼Œåæ˜ å‡ºAIè¡Œä¸šæ­£åœ¨ä»ç‹‚çƒ­æœŸè½¬å‘æ›´åŠ ç†æ€§å’ŒåŠ¡å®çš„é˜¶æ®µã€‚</p>
    </div>

    <div class="news-grid">
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=44848018" target="_blank">Why Boring Businesses Outlast AI Hype Cycles</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 9</span>
          </div>
          <div class="summary-text">æ¢è®¨åŸºç¡€ä¸šåŠ¡æ¨¡å¼æ¯”AIç‚’ä½œå‘¨æœŸæ›´å…·æŒä¹…æ€§çš„åŸå› </div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=27111719" target="_blank">Ask HN: What's the pain using current AI algorithms?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">è®¨è®ºå½“å‰AIç®—æ³•ä½¿ç”¨ä¸­çš„ç—›ç‚¹é—®é¢˜</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=35103021" target="_blank">The AI Crackpot Index</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 8</span>
          </div>
          <div class="summary-text">AIé¢†åŸŸè¿‡åº¦ç‚’ä½œå’Œè’è°¬è¨€è®ºçš„ç´¢å¼•</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=36233487" target="_blank">Ask HN: Is the rate of progress in AI exponential?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">Ask HN: Is the rate of progress in AI exponential?</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=36431356" target="_blank">Ask HN: Anyone concerned about NYC Local Law 144?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">è®¨è®ºå¯¹çº½çº¦å¸‚ç¬¬144å·åœ°æ–¹æ³•å¾‹ï¼ˆAIç›‘ç®¡ï¼‰çš„æ‹…å¿§</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://lekta.ai/blog/natural-language-processing-artificial-intelligence-machine-learning-bots-a-passing-trend-or-much-more" target="_blank">NLP, AI, ML, bots â€“ a passing trend or much more? What's your take on this?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 7</span>
          </div>
          <div class="summary-text">æ¢è®¨NLPã€AIã€MLå’Œæœºå™¨äººæŠ€æœ¯æ˜¯çŸ­æœŸè¶‹åŠ¿è¿˜æ˜¯é•¿æœŸå˜é©</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=44899994" target="_blank">Ask HN: What would you read to learn about "artificial intelligence"?</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">å¾æ±‚å­¦ä¹ äººå·¥æ™ºèƒ½çš„æ¨èé˜…è¯»ææ–™</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=12995049" target="_blank">Ask HN: Dipping my toes with artificial intelligence and what to expect? (CS)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 6</span>
          </div>
          <div class="summary-text">è®¡ç®—æœºç§‘å­¦ä¸“ä¸šå­¦ç”Ÿåˆæ¶‰AIé¢†åŸŸçš„æœŸæœ›å’Œå»ºè®®</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=401541" target="_blank">Common Lisp + Machine Learning Internship at Google (Mountain View, CA)</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">è°·æ­Œå±±æ™¯åŸCommon Lispä¸æœºå™¨å­¦ä¹ å®ä¹ æœºä¼š</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://www.yourobot.io/blog/uncategorized/the-next-bill-gates-or-albert-einstein-in-ai-artificial-intelligence-will-produce-the-god-algorithm-of-machine-learning-where-a-machine-will-be-able-to-do-and-learn-anything-by-its-self/" target="_blank">The Next Bill Gates or Albert Einstein in AI "Chris Clark" â€“ Yourobot</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 4</span>
          </div>
          <div class="summary-text">ä»‹ç»è¢«èª‰ä¸ºAIç•Œä¸‹ä¸€ä¸ªæ¯”å°”Â·ç›–èŒ¨æˆ–çˆ±å› æ–¯å¦çš„Chris Clark</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=42619160" target="_blank">Show HN: Startup Raising capital through Book Sales</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 3</span>
          </div>
          <div class="summary-text">å±•ç¤ºé€šè¿‡å›¾ä¹¦é”€å”®ç­¹é›†èµ„é‡‘çš„åˆåˆ›å…¬å¸</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="https://news.ycombinator.com/item?id=15140715" target="_blank">Bioinformatician</a></h3>
          <div class="news-meta">
            <span class="category industry">è¡Œä¸šåŠ¨æ€</span>
            <span>Hacker News</span>
            <span class="importance">é‡è¦åº¦: 3</span>
          </div>
          <div class="summary-text">ç”Ÿç‰©ä¿¡æ¯å­¦ç›¸å…³èŒä½æˆ–è®¨è®º</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27688v1" target="_blank">Continuous Autoregressive Language Models</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27680v1" target="_blank">PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Recent advances in vision-language models (VLMs) have enabled impressive
multimodal reasoning, yet most medical applications remain limited to 2D
imaging. In this work, we extend VLMs to 3D positron emission tomography and
computed tomography (PET/CT), a domain characterized by large volumetric data,
small and dispersed lesions, and lengthy radiology reports. We introduce a
large-scale dataset comprising over 11,000 lesion-level descriptions paired
with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid
rule-based and large language model (LLM) pipeline. Building upon this dataset,
we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,
CT, and lesion contours for spatially grounded report generation. PETAR bridges
global contextual reasoning with fine-grained lesion awareness, producing
clinically coherent and localized findings. Comprehensive automated and human
evaluations demonstrate that PETAR substantially improves PET/CT report
generation quality, advancing 3D medical vision-language understanding.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27671v1" target="_blank">MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Structure-based drug design (SBDD), which maps target proteins to candidate
molecular ligands, is a fundamental task in drug discovery. Effectively
aligning protein structural representations with molecular representations, and
ensuring alignment between generated drugs and their pharmacological
properties, remains a critical challenge. To address these challenges, we
propose MolChord, which integrates two key techniques: (1) to align protein and
molecule structures with their textual descriptions and sequential
representations (e.g., FASTA for proteins and SMILES for molecules), we
leverage NatureLM, an autoregressive model unifying text, small molecules, and
proteins, as the molecule generator, alongside a diffusion-based structure
encoder; and (2) to guide molecules toward desired properties, we curate a
property-aware dataset by integrating preference data and refine the alignment
process using Direct Preference Optimization (DPO). Experimental results on
CrossDocked2020 demonstrate that our approach achieves state-of-the-art
performance on key evaluation metrics, highlighting its potential as a
practical tool for SBDD.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27659v1" target="_blank">Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27655v1" target="_blank">Community Detection on Model Explanation Graphs for Explainable AI</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions
but often miss higher-order structure: sets of features that act in concert. We
propose Modules of Influence (MoI), a framework that (i) constructs a model
explanation graph from per-instance attributions, (ii) applies community
detection to find feature modules that jointly affect predictions, and (iii)
quantifies how these modules relate to bias, redundancy, and causality
patterns. Across synthetic and real datasets, MoI uncovers correlated feature
groups, improves model debugging via module-level ablations, and localizes bias
exposure to specific modules. We release stability and synergy metrics, a
reference implementation, and evaluation protocols to benchmark module
discovery in XAI.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27651v1" target="_blank">Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Modern deep neural networks (DNNs) are typically trained with a global
cross-entropy loss in a supervised end-to-end manner: neurons need to store
their outgoing weights; training alternates between a forward pass
(computation) and a top-down backward pass (learning) which is biologically
implausible. Alternatively, greedy layer-wise training eliminates the need for
cross-entropy loss and backpropagation. By avoiding the computation of
intermediate gradients and the storage of intermediate outputs, it reduces
memory usage and helps mitigate issues such as vanishing or exploding
gradients. However, most existing layer-wise training approaches have been
evaluated only on relatively small datasets with simple deep architectures. In
this paper, we first systematically analyze the training dynamics of popular
convolutional neural networks (CNNs) trained by stochastic gradient descent
(SGD) through an information-theoretic lens. Our findings reveal that networks
converge layer-by-layer from bottom to top and that the flow of information
adheres to a Markov information bottleneck principle. Building on these
observations, we propose a novel layer-wise training approach based on the
recently developed deterministic information bottleneck (DIB) and the
matrix-based R\'enyi's $\alpha$-order entropy functional. Specifically, each
layer is trained jointly with an auxiliary classifier that connects directly to
the output layer, enabling the learning of minimal sufficient task-relevant
representations. We empirically validate the effectiveness of our training
procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further
demonstrate its applicability to a practical task involving traffic sign
recognition. Our approach not only outperforms existing layer-wise training
baselines but also achieves performance comparable to SGD.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27646v1" target="_blank">VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Semantic segmentation of blood vessels is an important task in medical image
analysis, but its progress is often hindered by the scarcity of large annotated
datasets and the poor generalization of models across different imaging
modalities. A key aspect is the tendency of Convolutional Neural Networks
(CNNs) to learn texture-based features, which limits their performance when
applied to new domains with different visual characteristics. We hypothesize
that leveraging geometric priors of vessel shapes, such as their tubular and
branching nature, can lead to more robust and data-efficient models. To
investigate this, we introduce VessShape, a methodology for generating
large-scale 2D synthetic datasets designed to instill a shape bias in
segmentation models. VessShape images contain procedurally generated tubular
geometries combined with a wide variety of foreground and background textures,
encouraging models to learn shape cues rather than textures. We demonstrate
that a model pre-trained on VessShape images achieves strong few-shot
segmentation performance on two real-world datasets from different domains,
requiring only four to ten samples for fine-tuning. Furthermore, the model
exhibits notable zero-shot capabilities, effectively segmenting vessels in
unseen domains without any target-specific training. Our results indicate that
pre-training with a strong shape bias can be an effective strategy to overcome
data scarcity and improve model generalization in blood vessel segmentation.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27632v1" target="_blank">Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Graphic layout generation is a growing research area focusing on generating
aesthetically pleasing layouts ranging from poster designs to documents. While
recent research has explored ways to incorporate user constraints to guide the
layout generation, these constraints often require complex specifications which
reduce usability. We introduce an innovative approach exploiting user-provided
sketches as intuitive constraints and we demonstrate empirically the
effectiveness of this new guidance method, establishing the sketch-to-layout
problem as a promising research direction, which is currently under-explored.
To tackle the sketch-to-layout problem, we propose a multimodal
transformer-based solution using the sketch and the content assets as inputs to
produce high quality layouts. Since collecting sketch training data from human
annotators to train our model is very costly, we introduce a novel and
efficient method to synthetically generate training sketches at scale. We train
and evaluate our model on three publicly available datasets: PubLayNet,
DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art
constraint-based methods, while offering a more intuitive design experience. In
order to facilitate future sketch-to-layout research, we release O(200k)
synthetically-generated sketches for the public datasets above. The datasets
are available at https://github.com/google-deepmind/sketch_to_layout.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27630v1" target="_blank">Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Large Language Model (LLM) agents have recently shown strong potential in
domains such as automated coding, deep research, and graphical user interface
manipulation. However, training them to succeed on long-horizon,
domain-specialized tasks remains challenging. Current methods primarily fall
into two categories. The first relies on dense human annotations through
behavior cloning, which is prohibitively expensive for long-horizon tasks that
can take days or months. The second depends on outcome-driven sampling, which
often collapses due to the rarity of valid positive trajectories on
domain-specialized tasks. We introduce Apollo, a sampling framework that
integrates asynchronous human guidance with action-level data filtering.
Instead of requiring annotators to shadow every step, Apollo allows them to
intervene only when the agent drifts from a promising trajectory, by providing
prior knowledge, strategic advice, etc. This lightweight design makes it
possible to sustain interactions for over 30 hours and produces valuable
trajectories at a lower cost. Apollo then applies supervision control to filter
out sub-optimal actions and prevent error propagation. Together, these
components enable reliable and effective data collection in long-horizon
environments. To demonstrate the effectiveness of Apollo, we evaluate it using
InnovatorBench. Our experiments show that when applied to train the GLM-4.5
model on InnovatorBench, Apollo achieves more than a 50% improvement over the
untrained baseline and a 28% improvement over a variant trained without human
interaction. These results highlight the critical role of human-in-the-loop
sampling and the robustness of Apollo's design in handling long-horizon,
domain-specialized tasks.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27629v1" target="_blank">Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Open-weight bio-foundation models present a dual-use dilemma. While holding
great promise for accelerating scientific research and drug development, they
could also enable bad actors to develop more deadly bioweapons. To mitigate the
risk posed by these models, current approaches focus on filtering biohazardous
data during pre-training. However, the effectiveness of such an approach
remains unclear, particularly against determined actors who might fine-tune
these models for malicious use. To address this gap, we propose \eval, a
framework to evaluate the robustness of procedures that are intended to reduce
the dual-use capabilities of bio-foundation models. \eval assesses models'
virus understanding through three lenses, including sequence modeling,
mutational effects prediction, and virulence prediction. Our results show that
current filtering practices may not be particularly effective: Excluded
knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits
broader generalizability in sequence modeling. Furthermore, dual-use signals
may already reside in the pretrained representations, and can be elicited via
simple linear probing. These findings highlight the challenges of data
filtering as a standalone procedure, underscoring the need for further research
into robust safety and security strategies for open-weight bio-foundation
models.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27628v1" target="_blank">Validity Is What You Need</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">While AI agents have long been discussed and studied in computer science,
today's Agentic AI systems are something new. We consider other definitions of
Agentic AI and propose a new realist definition. Agentic AI is a software
delivery mechanism, comparable to software as a service (SaaS), which puts an
application to work autonomously in a complex enterprise setting. Recent
advances in large language models (LLMs) as foundation models have driven
excitement in Agentic AI. We note, however, that Agentic AI systems are
primarily applications, not foundations, and so their success depends on
validation by end users and principal stakeholders. The tools and techniques
needed by the principal users to validate their applications are quite
different from the tools and techniques used to evaluate foundation models.
Ironically, with good validation measures in place, in many cases the
foundation models can be replaced with much simpler, faster, and more
interpretable models that handle core logic. When it comes to Agentic AI,
validity is what you need. LLMs are one option that might achieve it.</div>
          
          
        </div>
      
        <div class="news-item">
          <h3><a href="http://arxiv.org/abs/2510.27623v1" target="_blank">Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning</a></h3>
          <div class="news-meta">
            <span class="category academic">å­¦æœ¯è®ºæ–‡</span>
            <span>ArXiv</span>
            <span class="importance">é‡è¦åº¦: 5</span>
          </div>
          <div class="summary-text">Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.</div>
          
          
        </div>
      
    </div>

    <div class="history-section">
        <h2>ğŸ“… å†å²æ—¥æŠ¥ç›®å½•</h2>
        <ul class="history-list">
          <li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-11-02.html" target="_blank">2025-11-02</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-11-01.html" target="_blank">2025-11-01</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-31.html" target="_blank">2025-10-31</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-30.html" target="_blank">2025-10-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-29.html" target="_blank">2025-10-29</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-28.html" target="_blank">2025-10-28</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-27.html" target="_blank">2025-10-27</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-26.html" target="_blank">2025-10-26</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-25.html" target="_blank">2025-10-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-24.html" target="_blank">2025-10-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-23.html" target="_blank">2025-10-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-22.html" target="_blank">2025-10-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-21.html" target="_blank">2025-10-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-20.html" target="_blank">2025-10-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-19.html" target="_blank">2025-10-19</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-18.html" target="_blank">2025-10-18</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-17.html" target="_blank">2025-10-17</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-16.html" target="_blank">2025-10-16</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-15.html" target="_blank">2025-10-15</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-14.html" target="_blank">2025-10-14</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-13.html" target="_blank">2025-10-13</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-12.html" target="_blank">2025-10-12</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-11.html" target="_blank">2025-10-11</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-10.html" target="_blank">2025-10-10</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-09.html" target="_blank">2025-10-09</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-08.html" target="_blank">2025-10-08</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-07.html" target="_blank">2025-10-07</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-06.html" target="_blank">2025-10-06</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-05.html" target="_blank">2025-10-05</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-04.html" target="_blank">2025-10-04</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-03.html" target="_blank">2025-10-03</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-02.html" target="_blank">2025-10-02</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-10-01.html" target="_blank">2025-10-01</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-30.html" target="_blank">2025-09-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-29.html" target="_blank">2025-09-29</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-28.html" target="_blank">2025-09-28</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-27.html" target="_blank">2025-09-27</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-26.html" target="_blank">2025-09-26</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-25.html" target="_blank">2025-09-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-24.html" target="_blank">2025-09-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-23.html" target="_blank">2025-09-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-22.html" target="_blank">2025-09-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-21.html" target="_blank">2025-09-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-20.html" target="_blank">2025-09-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-19.html" target="_blank">2025-09-19</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-18.html" target="_blank">2025-09-18</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-17.html" target="_blank">2025-09-17</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-16.html" target="_blank">2025-09-16</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-15.html" target="_blank">2025-09-15</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-14.html" target="_blank">2025-09-14</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-13.html" target="_blank">2025-09-13</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-12.html" target="_blank">2025-09-12</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-11.html" target="_blank">2025-09-11</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-10.html" target="_blank">2025-09-10</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-09.html" target="_blank">2025-09-09</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-08.html" target="_blank">2025-09-08</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-07.html" target="_blank">2025-09-07</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-06.html" target="_blank">2025-09-06</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-05.html" target="_blank">2025-09-05</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-09-04.html" target="_blank">2025-09-04</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-30.html" target="_blank">2025-08-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-25.html" target="_blank">2025-08-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-24.html" target="_blank">2025-08-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-23.html" target="_blank">2025-08-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-22.html" target="_blank">2025-08-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-21.html" target="_blank">2025-08-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-20.html" target="_blank">2025-08-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-17.html" target="_blank">2025-08-17</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-15.html" target="_blank">2025-08-15</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-14.html" target="_blank">2025-08-14</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-13.html" target="_blank">2025-08-13</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-11.html" target="_blank">2025-08-11</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-10.html" target="_blank">2025-08-10</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-08.html" target="_blank">2025-08-08</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-07.html" target="_blank">2025-08-07</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-06.html" target="_blank">2025-08-06</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-05.html" target="_blank">2025-08-05</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-04.html" target="_blank">2025-08-04</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-03.html" target="_blank">2025-08-03</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-02.html" target="_blank">2025-08-02</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-08-01.html" target="_blank">2025-08-01</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-31.html" target="_blank">2025-07-31</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-30.html" target="_blank">2025-07-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-29.html" target="_blank">2025-07-29</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-27.html" target="_blank">2025-07-27</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-26.html" target="_blank">2025-07-26</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-25.html" target="_blank">2025-07-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-24.html" target="_blank">2025-07-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-23.html" target="_blank">2025-07-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-22.html" target="_blank">2025-07-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-21.html" target="_blank">2025-07-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-20.html" target="_blank">2025-07-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-19.html" target="_blank">2025-07-19</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-18.html" target="_blank">2025-07-18</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-17.html" target="_blank">2025-07-17</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-16.html" target="_blank">2025-07-16</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-15.html" target="_blank">2025-07-15</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-13.html" target="_blank">2025-07-13</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-12.html" target="_blank">2025-07-12</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-11.html" target="_blank">2025-07-11</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-10.html" target="_blank">2025-07-10</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-09.html" target="_blank">2025-07-09</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-08.html" target="_blank">2025-07-08</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-07.html" target="_blank">2025-07-07</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-06.html" target="_blank">2025-07-06</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-05.html" target="_blank">2025-07-05</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-04.html" target="_blank">2025-07-04</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-03.html" target="_blank">2025-07-03</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-02.html" target="_blank">2025-07-02</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-07-01.html" target="_blank">2025-07-01</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-30.html" target="_blank">2025-06-30</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-29.html" target="_blank">2025-06-29</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-28.html" target="_blank">2025-06-28</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-27.html" target="_blank">2025-06-27</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-26.html" target="_blank">2025-06-26</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-25.html" target="_blank">2025-06-25</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-24.html" target="_blank">2025-06-24</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-23.html" target="_blank">2025-06-23</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-22.html" target="_blank">2025-06-22</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-21.html" target="_blank">2025-06-21</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-20.html" target="_blank">2025-06-20</a></li><li><a href="https://wjjzl519.github.io/ai-news-daily/daily/2025-06-19.html" target="_blank">2025-06-19</a></li>
        </ul>
      </div>

    <div class="footer">
      ğŸ”„ ç”± Cloudflare Workers + DeepSeek è‡ªåŠ¨ç”Ÿæˆ | æ›´æ–°æ—¶é—´: 2025/11/3 22:02:22
    </div>
  </div>
</body>
</html>